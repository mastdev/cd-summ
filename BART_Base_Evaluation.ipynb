{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "# Preprocess the input and output text\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['text'], max_length=1024, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(examples['summary'], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    # Set labels for training\n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6668/6668 [1:31:04<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.25456955875129933,\n",
       " 'rouge2': 0.16404162263024455,\n",
       " 'rougeL': 0.2139045876625417}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./entertainment_test.csv')\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge_scores = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    inputs = tokenizer(row['text'], return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4)\n",
    "    predicted_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    score = scorer.score(row['summary'], predicted_summary)\n",
    "    rouge_scores.append(score)\n",
    "\n",
    "average_scores_arch = {metric: sum([score[metric].fmeasure for score in rouge_scores]) / len(rouge_scores) for metric in rouge_scores[0]}\n",
    "average_scores_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 419/419 [05:42<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.24576866517263188,\n",
       " 'rouge2': 0.1547942113323618,\n",
       " 'rougeL': 0.2059425862827461}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./architecture_test.csv')\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge_scores = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    inputs = tokenizer(row['text'], return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4)\n",
    "    predicted_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    score = scorer.score(row['summary'], predicted_summary)\n",
    "    rouge_scores.append(score)\n",
    "\n",
    "average_scores_arch = {metric: sum([score[metric].fmeasure for score in rouge_scores]) / len(rouge_scores) for metric in rouge_scores[0]}\n",
    "average_scores_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1482/1482 [20:15<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.2333310144247373,\n",
       " 'rouge2': 0.14703136749240425,\n",
       " 'rougeL': 0.1979435435894947}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./food_test.csv')\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge_scores = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    inputs = tokenizer(row['text'], return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4)\n",
    "    predicted_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    score = scorer.score(row['summary'], predicted_summary)\n",
    "    rouge_scores.append(score)\n",
    "\n",
    "average_scores_arch = {metric: sum([score[metric].fmeasure for score in rouge_scores]) / len(rouge_scores) for metric in rouge_scores[0]}\n",
    "average_scores_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6163/6163 [1:24:47<00:00,  1.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.23407144187032236,\n",
       " 'rouge2': 0.14129025812317933,\n",
       " 'rougeL': 0.19308868959917938}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./sports_test.csv')\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge_scores = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    inputs = tokenizer(row['text'], return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4)\n",
    "    predicted_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    score = scorer.score(row['summary'], predicted_summary)\n",
    "    rouge_scores.append(score)\n",
    "\n",
    "average_scores_arch = {metric: sum([score[metric].fmeasure for score in rouge_scores]) / len(rouge_scores) for metric in rouge_scores[0]}\n",
    "average_scores_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5448/5448 [1:14:13<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.25317686616376717,\n",
       " 'rouge2': 0.16564610536302368,\n",
       " 'rougeL': 0.21556226523124777}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./technology_test.csv')\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge_scores = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    inputs = tokenizer(row['text'], return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4)\n",
    "    predicted_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    score = scorer.score(row['summary'], predicted_summary)\n",
    "    rouge_scores.append(score)\n",
    "\n",
    "average_scores_tech = {metric: sum([score[metric].fmeasure for score in rouge_scores]) / len(rouge_scores) for metric in rouge_scores[0]}\n",
    "average_scores_tech"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
